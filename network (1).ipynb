{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"network.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yyhClwkFK-Ry","colab_type":"text"},"cell_type":"markdown","source":["# 3D Convolutional Neural Network"]},{"metadata":{"id":"QRK0uGbiLGOq","colab_type":"text"},"cell_type":"markdown","source":["# Dependencies"]},{"metadata":{"id":"5tSZHFeXLMm4","colab_type":"text"},"cell_type":"markdown","source":["## Installing"]},{"metadata":{"id":"yjPXpHJsLQxs","colab_type":"code","colab":{}},"cell_type":"code","source":["# Installing depedencies for Colab\n","# !pip install package"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"MkoGLH_Tj5wn"},"cell_type":"markdown","source":["## Importing"]},{"metadata":{"colab_type":"code","id":"ORj09gnrj5wp","colab":{}},"cell_type":"code","source":["#### Our imports ####\n","\n","from google.colab import drive\n","import h5py\n","\n","#### From SR ####\n","\n","import os\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","from torchvision import datasets\n","from torchvision import transforms\n","\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","\n","if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tyLC7l4yPrRS","colab_type":"text"},"cell_type":"markdown","source":["## Mounting our Google Drive"]},{"metadata":{"id":"cqZTHZInPuYZ","colab_type":"code","outputId":"812b5df0-e4d9-474f-80c3-3455f19b6378","executionInfo":{"status":"ok","timestamp":1554330350440,"user_tz":300,"elapsed":36679,"user":{"displayName":"JIAHUI JIANG","photoUrl":"","userId":"04815942212736010474"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["drive.mount('/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"DVBJu6R_SUiH","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"I6hghKPxj5w0"},"cell_type":"markdown","source":["# Model Settings"]},{"metadata":{"colab_type":"code","id":"NnT0sZIwj5wu","colab":{}},"cell_type":"code","source":["###############################\n","### NO NEED TO CHANGE THIS CELL\n","###############################\n","\n","\n","#-------------------------\n","### SETTINGS\n","#-------------------------\n","\n","# Hyperparameters\n","RANDOM_SEED = 1\n","LEARNING_RATE = 0.001\n","BATCH_SIZE = 256\n","NUM_EPOCHS = 20\n","\n","# Architecture\n","NUM_FEATURES = 32*32\n","NUM_CLASSES = 10\n","\n","# Other\n","DEVICE = \"cuda:0\""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"RAodboScj5w6"},"cell_type":"markdown","source":["# Dataset : [3D MNIST](https://www.kaggle.com/daavoo/3d-mnist)"]},{"metadata":{"id":"_vSkn4MKQSkL","colab_type":"code","outputId":"00afedf3-e542-4162-b697-383360b21c3b","executionInfo":{"status":"ok","timestamp":1554329677820,"user_tz":300,"elapsed":224,"user":{"displayName":"Zheming Lian","photoUrl":"","userId":"14963944757674792354"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"L_KNCZHn8Gis","colab_type":"code","outputId":"d61e1fee-4c10-4384-df40-435ed7ddfbed","executionInfo":{"status":"error","timestamp":1554330505480,"user_tz":300,"elapsed":1481,"user":{"displayName":"JIAHUI JIANG","photoUrl":"","userId":"04815942212736010474"}},"colab":{"base_uri":"https://localhost:8080/","height":525}},"cell_type":"code","source":["with h5py.File(\"/gdrive/Team Drives/479 group project/Data/train_point_clouds.h5\", \"r\") as hf:    \n","    X_train = hf[\"X_train\"][:]\n","    y_train = hf[\"y_train\"][:]    \n","    X_test = hf[\"X_test\"][:]  \n","    y_test = hf[\"y_test\"][:] \n","print (\"x_train shape: \", x_train.shape)\n","print (\"y_train shape: \", y_train.shape)\n","\n","print (\"x_test shape:  \", x_test.shape)\n","print (\"y_test shape:  \", y_test.shape)\n","\n","# # Note transforms.ToTensor() scales input images\n","# # to 0-1 range\n","# train_dataset = datasets.CIFAR10(root='data', \n","#                                  train=True, \n","#                                  transform=transforms.ToTensor(),\n","#                                  download=True)\n","\n","# test_dataset = datasets.CIFAR10(root='data', \n","#                                 train=False, \n","#                                 transform=transforms.ToTensor())\n","\n","\n","# train_loader = DataLoader(dataset=train_dataset, \n","#                           batch_size=BATCH_SIZE, \n","#                           num_workers=8,\n","#                           shuffle=True)\n","\n","# test_loader = DataLoader(dataset=test_dataset, \n","#                          batch_size=BATCH_SIZE,\n","#                          num_workers=8,\n","#                          shuffle=False)\n","\n","# # Checking the dataset\n","# for images, labels in train_loader:  \n","#     print('Image batch dimensions:', images.shape)\n","#     print('Image label dimensions:', labels.shape)\n","#     break\n","\n","# # Checking the dataset\n","# for images, labels in train_loader:  \n","#     print('Image batch dimensions:', images.shape)\n","#     print('Image label dimensions:', labels.shape)\n","#     break"],"execution_count":8,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-902a2997de01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gdrive/Team Drives/479 group project/Data/train_point_clouds.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'X_train' doesn't exist)\""]}]},{"metadata":{"id":"YN6NZwH88Giw","colab_type":"code","colab":{}},"cell_type":"code","source":["###############################\n","### NO NEED TO CHANGE THIS CELL\n","###############################\n","\n","def compute_epoch_loss(model, data_loader):\n","    model.eval()\n","    curr_loss, num_examples = 0., 0\n","    with torch.no_grad():\n","        for features, targets in data_loader:\n","            features = features.to(DEVICE)\n","            targets = targets.to(DEVICE)\n","            logits, probas = model(features)\n","            loss = F.cross_entropy(logits, targets, reduction='sum')\n","            num_examples += targets.size(0)\n","            curr_loss += loss\n","\n","        curr_loss = curr_loss / num_examples\n","        return curr_loss\n","\n","\n","def compute_accuracy(model, data_loader, device):\n","    model.eval()\n","    correct_pred, num_examples = 0, 0\n","    for i, (features, targets) in enumerate(data_loader):\n","            \n","        features = features.to(device)\n","        targets = targets.to(device)\n","\n","        logits, probas = model(features)\n","        _, predicted_labels = torch.max(probas, 1)\n","        num_examples += targets.size(0)\n","        correct_pred += (predicted_labels == targets).sum()\n","    return correct_pred.float()/num_examples * 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sHXB0fvKFpGN","colab_type":"text"},"cell_type":"markdown","source":["# Network Architecture"]},{"metadata":{"id":"Gg8YcW2l8Gi6","colab_type":"code","colab":{}},"cell_type":"code","source":["##########################\n","### MODEL\n","##########################\n","\n","class C3D(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(C3D, self).__init__()\n","\n","        self.conv1 =  nn.Conv3d(3, 64,    kernel_size=5, stride=1, padding=2)\n","        \n","        \n","        \n","        # ... <add the remaining convolutional layers\n","        # and fully connected layers ...\n","        \n","        self.linear3 = nn.Linear(4096, num_classes)  \n","        \n","        # Convolutional Layers\n","        \n","        self.group1 = nn.Sequential(\n","            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)))\n","        \n","        self.group2 = nn.Sequential(\n","            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n","        \n","        self.group3 = nn.Sequential(\n","            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n","        \n","        self.group4 = nn.Sequential(\n","            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n","        \n","        self.group5 = nn.Sequential(\n","            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n","        \n","        # Fully Connected Layers\n","        \n","        self.fc1 = nn.Sequential(\n","            nn.Linear(512 * 3 * 3, 2048),\n","            nn.ReLU(),\n","            nn.Dropout(0.5))\n","        \n","        self.fc2 = nn.Sequential(\n","            nn.Linear(2048, 2048),\n","            nn.ReLU(),\n","            nn.Dropout(0.5))\n","        \n","        self.fc3 = nn.Sequential(\n","            nn.Linear(2048, 32))\n","\n","        # Summarizing Each Group\n","        \n","        self._features = nn.Sequential(\n","            self.group1,\n","            self.group2,\n","            self.group3,\n","            self.group4,\n","            self.group5\n","        )\n","\n","        self._classifier = nn.Sequential(\n","            self.fc1,\n","            self.fc2\n","        )\n","        \n","\n","    def forward(self, x):\n","        \n","        out = self._features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self._classifier(out)\n","        \n","        logits = self.fc3(out)\n","        probas = F.softmax(logits, dim=1)\n","        \n","        return logits, probas\n","\n","    \n","torch.manual_seed(RANDOM_SEED)\n","\n","model1 = ConvNet1(NUM_CLASSES)\n","model1.to(DEVICE)\n","\n","optimizer = torch.optim.Adam(model1.parameters(), lr=LEARNING_RATE)  "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Dzh3ROmRj5w7","scrolled":false,"colab":{}},"cell_type":"code","source":["###############################\n","### NO NEED TO CHANGE THIS CELL\n","###############################\n","\n","def train(model, train_loader, test_loader):\n","\n","    minibatch_cost, epoch_cost = [], []\n","    start_time = time.time()\n","    for epoch in range(NUM_EPOCHS):\n","\n","        model.train()\n","        for batch_idx, (features, targets) in enumerate(train_loader):\n","\n","            features = features.to(DEVICE)\n","            targets = targets.to(DEVICE)\n","\n","            ### FORWARD AND BACK PROP\n","            logits, probas = model(features)\n","            cost = F.cross_entropy(logits, targets)\n","            optimizer.zero_grad()\n","\n","            cost.backward()\n","            minibatch_cost.append(cost)\n","\n","            ### UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            ### LOGGING\n","            if not batch_idx % 150:\n","                print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n","                       %(epoch+1, NUM_EPOCHS, batch_idx, \n","                         len(train_loader), cost))\n","\n","    \n","        with torch.set_grad_enabled(False): # save memory during inference\n","            print('Epoch: %03d/%03d | Train: %.3f%%' % (\n","                  epoch+1, NUM_EPOCHS, \n","                  compute_accuracy(model, train_loader, device=DEVICE)))\n","            \n","            cost = compute_epoch_loss(model, train_loader)\n","            epoch_cost.append(cost)\n","\n","        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n","\n","    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n","\n","\n","    with torch.set_grad_enabled(False): # save memory during inference\n","        print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n","\n","    print('Total Time: %.2f min' % ((time.time() - start_time)/60))\n","    \n","    return minibatch_cost, epoch_cost\n","    \n","\n","minibatch_cost, epoch_cost = train(model1, train_loader, test_loader)\n","\n","\n","plt.plot(range(len(minibatch_cost)), minibatch_cost)\n","plt.ylabel('Cross Entropy')\n","plt.xlabel('Minibatch')\n","plt.show()\n","\n","plt.plot(range(len(epoch_cost)), epoch_cost)\n","plt.ylabel('Cross Entropy')\n","plt.xlabel('Epoch')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z_RZTyNO8Gi_","colab_type":"code","colab":{}},"cell_type":"code","source":["del model1  # to save memory if you don't use it anymore"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Aj_XBpwcN25b","colab_type":"text"},"cell_type":"markdown","source":["# Experimental Results"]}]}